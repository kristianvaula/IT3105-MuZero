environment: 'snakepac' # 'snakepac' or other when implemented
logging_config: 
  use_wandb: True
  wandb_project: 'MuZero'
  wandb_name: 'snakepac'
  load_model: False
  save_model: True
  checkpoint_interval: 100

snakepac:
  num_episodes: 1000 # How many episodes to train the model 
  num_episode_step: 20 # How many steps to take in each episode 
  training_interval: 25 # How often to train the model 
  buffer_size: 75  # How many episodes to store in the buffer 
  batch_size: 10   # Parameter K in the bptt, how many episodes to unroll from the buffer
  world_length: 8
  seed: 42
  action_space: &action_size_snakepac 3
  uMCTS:
    num_searches: 20
    max_depth: 8
    ucb_constant: 1.0
    discount_factor: 0.95
  
  network:
    iteration: ""
    state_window: 1 # How many episodes we use to compute initial abstract state 
    hidden_state_size: &hidden_size_snakepac 32
    representation:
    - type: linear
      in_features: 9 # World length * state_window + state_window (corresponding actions)
      out_features: 128
      activation: relu
    - type: linear
      in_features: 128
      out_features: 128
    - type: linear
      in_features: 128
      out_features: 64
    - type: linear
      in_features: 64
      out_features: *hidden_size_snakepac

    prediction:
      - type: linear
        in_features: *hidden_size_snakepac
        out_features: 64
        activation: relu
      - type: linear
        in_features: 64
        out_features: 64
      - type: linear
        in_features: 64
        out_features: 32
      # second to last layer is the policy layer
      - type: linear
        in_features: 32
        out_features: *action_size_snakepac
        activation: softmax
      # last layer is the value layer
      - type: linear
        in_features: 32
        out_features: 1

    dynamics:
      - type: linear
        in_features: 33 # &hidden_size_snakepac + 1 (action)
        out_features: 128
        activation: relu
      - type: linear
        in_features: 128
        out_features: 128
      - type: linear
        in_features: 128
        out_features: 64
      # second to last layer is the hidden state layer
      - type: linear
        in_features: 64
        out_features: *hidden_size_snakepac
      # last layer is the reward layer
      - type: linear
        in_features: 64
        out_features: 1


  

riverraid:
  num_episodes: 100
  num_simulations: 100
  training_interval: 10
  batch_size: 32

  skip_frames: 4
  action_frames: 4
  mcts:
    num_searches: 50
    max_depth: 10
    ucb_constant: 1.0
    discount_factor: 0.95

logging:

network:
  # for snakepac
  iteration: ""
  representation:
  - type: linear
    in_features: "" # Will be set by the environment
    out_features: 
    activation: relu
  - type: linear
    in_features: 128
    out_features: 64

  prediction:
    - type: linear
      in_features: 64
      out_features: 64
      activation: relu
    - type: linear
      in_features: 64
      out_features: 10

  dynamics:
    - type: linear
      in_features: 68
      out_features: 128
      activation: relu
    - type: linear
      in_features: 128
      out_features: 64
